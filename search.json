[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Τα άρθρα μου",
    "section": "",
    "text": "Ταξινόμηση κατά\n       Προκαθορισμένη\n         \n          Τίτλος\n        \n         \n          Ημερομηνία - Παλαιότερο\n        \n         \n          Ημερομηνία - Νεότερο\n        \n     \n  \n\n\n\n\n\nΚαμία αντιστοίχιση"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Τα προσωπικά μου Project",
    "section": "",
    "text": "Οι ταινίες του Film Pit σε γραφήματα\n\n\nΣυγκέντρωση πληροφοριών για τις ταινίες του podcast “The Film Pit” με την γλώσσα Python.\n\n\n\n2022-10-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nΔασικές πυρκαγιές στην Ελλάδα\n\n\nΠαρουσίαση των δασικών πυρκαγιών στην Ελλάδα από το 2000 έως το 2021 με πηγή το αρχείο του Πυροσβεστικού Σώματος.\n\n\n\n2022-09-28\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nΗ προσωπική μου ιστοσελίδα\n\n\nΜία προσωπική ιστοσελίδα που θα φιλοξενεί τα άρθρα μου σχετικά με την Επιστήμη των Δεδομένων.\n\n\n\n2022-09-20\n\n\n\n\n\n\n\n\nΚαμία αντιστοίχιση"
  },
  {
    "objectID": "portfolio/my_website/my_website.html",
    "href": "portfolio/my_website/my_website.html",
    "title": "Η προσωπική μου ιστοσελίδα",
    "section": "",
    "text": "Το πρώτο project για το οποίο θα ήθελα να αναφερθώ είναι ο ιστότοπος όπου διαβάζετε αυτές τις γραμμές. Θεωρείται καλή πρακτική η δημοσίευση άρθρων, συγχρόνως με την εκμάνθηση προγραμματισμού ή της επιστήμης των δεδομένων (Data Science). Ο μαθητής επαναλαμβάνει το ύλικο που διδάκτηκε και βελτιώνει την ικανότητα του να παρουσιάζει την δουλειά του στο κοινό. Ενώ μια ήδη καθιερωμένη πλατφόρμα όπως το Medium ή το Wordpress είναι μία εύκολη επιλογή, ήθελα να δημιουργήσω κάτι από την αρχή για να δοκιμάσω τις ικανότητες μου στον προγραμματισμό με Python.\nΕίχα την τύχη να παρακολουθήσω μία σειρά διαδικτυακών μαθημάτων σχετικά με την ανάπτυξη ιστοσελίδων χρησιμοποιώντας HTML, CSS, JavaScript και την βιβλιοθήκη Flask της Python από την ελληνική πλατφόρμα Mathesis, τμήμα των Πανεπιστημιακών Εκδόσεων Κρήτης. Το Flask είναι σχεδιασμένο για να σας βοηθήσει να δημιουργήσετε το backend μίας ιστοσελίδας με όσο το δυνατόν λιγότερους υπολογιστικού πόρους. Υποστηρίζει επεκτάσεις με τις οποίες μπορείτε να προσθέσετε δυνατότητες σύμφωνα με τις ανάγκες σας. Χρησιμοποιείτε από γνωστές ιστοσελίδες όπως το Pinterest και το LinkedIn. Δύο χρήσιμα βιβλία για την εκμάθηση Flask είναι Flask Web Development: Developing Web Applications with Python του Miguel Grinberg και το Mastering Flask Web Development των Daniel Gaspar και Jack Stouffer στο Packt, όπως επίσης και το ιστολόγιο του Grinberg https://blog.miguelgrinberg.com.\nΣυνδιάζοντας την με την επέκταση Flask-Babel υπάρχει υποστήριξη για πολλαπλές γλώσσες. Στην πρώτη έκδοση της ιστοσελίδας, χρησιμοποίησα επίσης το Flask-CKEditor. Το CKEditor είναι ένα πρόγραμμα επεξεργασίας κειμένου απευθείας εντός των ιστοσελίδων και χρησιμοποιείτε από πολλά ιστολόγια και εφαρμογές λήψης σημειώσεων. Αν και το CKEditor είναι ένας ισχυρός επεξεργαστής κειμένου, κατάλαβα γρήγορα ότι χρειαζόμουν κάτι που να προσανατολίζεται στην επιστήμη των δεδομένων. Σε προγράμματα μαθημάτων ανάλυσης δεδομένων είναι συχνή η χρήση των Jupyter Notebooks. Είναι μία μορφή σημειωματάριου όπου μπορείς να συμπεριλάβεις κείμενο(markdown), κώδικα(Python, R, Julia, SQL κ.α.) και γραφήματα στην ανάλυση σου. Ωστόσο η τελική μορφή του δεν είναι κατάλληλη για δημοσίευση σε ένα άρθρο ή την δημιουργία ενός report. Αυτό έρχεται να λύσει το Quarto.\nTo Quarto είναι ένα πρόγραμμα ανοικτού κώδικα που βασίζεται στο Pandoc. Επιτρέπει την δημοσίευση άρθρων, αναφορών, παρουσιάσεων και βιβλίων υψηλής ποιότητας με την μετατροπή αρχείων markdown και Jupyter σε HTML, αρχεία pdf, word, epub κ.α. Μπορεί να ενσωματωθεί με εργαλεία όπως το VS Code, Jupyter Lab, R Studio ή ακόμη και με ένα απλό πρόγραμμα επεξεργασίας κειμένου. Με το Quarto επίσης μπορείτε να δημιουργήσετε ένα στατικό ιστότοπο και να το φιλοξενήσετε σε πλατφόρμες όπως το GitHub pages, το Netlify, RStudio Connnect. Δεν χρειάζεται καν να γνωρίζετε βιβλιοθήκες web development όπως το Flask που αναφέρθηκε παραπάνω. Ίσως χρειαστεί η γνώση βασικής HTML και CSS για να τροποποιήσετε την εμφάνιση του σύμφωνα με τις προτιμήσεις σας. Εκτός από την επίσημη τεκμηρίωση, υπάρχουν δύο εξαιρετικά άρθρα για το Quarto, το πρώτο αφορά τη δημιουργία ενός ιστολογίου και το δεύτερο αφορά την υποστήριξη διπλής γλώσσας.\nΈτσι συνδύασα τον αρχικό μου ιστότοπο με τον Flask το οποίο φιλοξενείτε στην πλατφόρμα pythonanywhere, με τον στατικό του Quarto που φιλοξενείτε στο GitHub pages. Ο πρώτος είναι υπεύθυνος για το ευρετήριο και τις βασικές σελίδες πληροφοριών (Contact, Blog, Index) με τον δεύτερο να φιλοξενεί τα άρθρα μου που έχουν δημιουργηθεί είτε με αρχεία markdown είτε με jupyter notebooks."
  },
  {
    "objectID": "portfolio/wildfires/helper.html",
    "href": "portfolio/wildfires/helper.html",
    "title": "Δασικές πυρκαγιές στην Ελλάδα",
    "section": "",
    "text": "Με την πάροδο των ετών περισσότερες πληροφορίες προστίθενται στα αρχεία.\n2000-2008\n\nΥπηρεσία (fire_service)\nΔασαρχείο (forest_department)\nΠεριοχή-Τοποθεσία (location)\nΝομός (district):\nΗμερ/νία Έναρξης (start_date)\nΏρα Έναρξης (start_time)\nΗμερ/νία Κατάσβεσης (end_date)\nΏρα Κατάσβεσης (end_time)\n\nΠαρακάτω κατηγοριοποιούνται οι εκτάσεις που κάηκαν σε στρέμματα.\n\nΔάση (forest)\nΔασική Έκταση (forest_area): Έπιπλέον δασική έκταση που καταστράφηκε.\nΆλση (grove)\nΧορτ/κές Εκτάσεις (grassland)\nΚαλάμια - Βάλτοι (marsh)\nΓεωργικές Εκτάσεις (crop_fields)\nΥπολλείματα Καλλιεργιών (crop_residues)\nΣκουπιδότοποι (dumps)\n\n2009-2010\n\nΔήμος (borough)\n\n2011\nΤο 2011 προστέθηκαν πληροφορίες για το ανθρώπινο δυναμικό και τα οχήματα που συμμετείχαν στην πυρόσβεση\n\nΠΥΡΟΣ, ΣΩΜΑ (firemen)\nΠΕΖΟΠΟΡΑ ΤΜΗΜΑΤΑ (patrol)\nΕΘΕΛΟΝΤΕΣ (volunteers)\nΣΤΡΑΤΟΣ (military)\nΑΛΛΕΣ ΔΥΝΑΜΕΙΣ (other_groups)\nΠΥΡΟΣ. ΟΧΗΜ. (fire_trucks): Πυροσβεστικά οχήματα.\nΟΧΗΜ. ΟΤΑ (local_authorities): Οχήματα της τοπικής αυτοδιοίκησης.\nΒΥΤΙΟΦΟΡΑ (private_trucks)\nΜΗΧΑΝΗΜΑΤΑ (other_trucks)\nΕΛΙΚΟΠΤΕΡΑ (helicopters)\nΑ/Φ CL415 (canadair_new): Canadair CL-415, το νεότερο μοντέλο του σκάφους πυρόσβεσης.\nΑ/Φ CL215 (canadair_old): Canadair CL-215, το παλαιότερο μοντέλο του σκάφους πυρόσβεσης.\nΑ/Φ PZL (pzl): PZL-Mielec, μονοκινητήρια αγροτικά αεροσκάφη.\nΑ/Φ GRU. (gru): Άλλου είδους αεροσκάφη\n\n2012-2019\n\nΔιεύθυνση (address)\n\n2020-2021\n\nΑ/Α ΕΓΓΡΑΦΗΣ - Α/Α ENGAGE\nX-ENGAGE / Y-ENGAGE (Γεωγρ. μήκος/πλάτος)\n\n\n# Εισάγουμε τις βιβλιοθήκες numpy και pandas\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "portfolio/wildfires/helper.html#προετοιμασία-του-αρχείου-ετών-2000-2012",
    "href": "portfolio/wildfires/helper.html#προετοιμασία-του-αρχείου-ετών-2000-2012",
    "title": "Δασικές πυρκαγιές στην Ελλάδα",
    "section": "Προετοιμασία του αρχείου ετών 2000-2012",
    "text": "Προετοιμασία του αρχείου ετών 2000-2012\nΤο πρώτο φύλλο excel περιέχει πληροφορίες για τα έτη 2000 - 2012, κάθε φύλλο και ένα έτος.\n\ndata = pd.ExcelFile('datasets/Dasikes_Pyrkagies_2000-2012.xls')\nprint(data.sheet_names)\n\n['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012']\n\n\nΠαρακάτω είναι ένα δείγμα 5 τυχαίων καταγραφών.\n\ndata.parse('2000', skiprows=1).sample(5)\n\n\n\n\n\n  \n    \n      \n      Υπηρεσία\n      Δασαρχείο\n      Περιοχή - Τοποθεσία\n      Νομός\n      Ημερ/νία Έναρξης\n      Ώρα Έναρξης\n      Ημερ/νία Κατασβεσης\n      Ώρα Κατάσβεσης\n      Δάση\n      Δασική Έκταση\n      Άλση\n      Χορτ/κές Εκτάσεις\n      Καλάμια - Βάλτοι\n      Γεωργικές Εκτάσεις\n      Υπολλείματα Καλλιεργειών\n      Σκουπιδότοποι\n    \n  \n  \n    \n      10506\n      Π.Υ  ΘΗΒΑΣ\n      NaN\n      89 ΧΙΛ.ΑΘΗΝΩΝ-ΛΑΜΙΑΣ\n      ΒΟΙΩΤΙΑΣ\n      2000-05-22\n      15:34:00\n      2000-05-22 00:00:00\n      16:30:00\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      4737\n      Π.Υ. ΑΡΤΑΣ\n      NaN\n      ΑΡΤΑ ΤΕΤΡΑΦΥΛΛΙΑΣ\n      ΑΡΤΑΣ\n      2000-12-10\n      13:39:00\n      2000-12-10 00:00:00\n      15:46:00\n      0.0\n      0.1\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      10611\n      Π.Υ. ΧΑΛΚΙΔΑΣ\n      ΧΑΛΚΙΔΑΣ\n      ΔΗΜ.ΜΕΣΣΑΠΙΩΝ ΠΕΡ.ΠΑΛΙΟΚΗΠΑΡΙΣΩ\n      ΕΥΒΟΙΑΣ\n      2000-06-20\n      18:00:00\n      2000-06-21 00:00:00\n      00:07:00\n      20.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      7011\n      Π.Υ. ΑΙΓΙΟΥ\n      ΑΙΓΙΟΥ\n      ΒΟΥΛΩΜΕΝΟ\n      ΑΧΑΙΑΣ\n      2000-07-05\n      13:15:00\n      2000-07-06 00:00:00\n      10:00:00\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2.0\n    \n    \n      6295\n      Π.Υ. ΚΑΡΔΙΤΣΑΣ\n      ΚΑΡΔΙΤΣΑΣ\n      ΜΑΓΟΥΛΑ\n      ΚΑΡΔΙΤΣΑΣ\n      2000-09-02\n      11:05:00\n      2000-09-02 00:00:00\n      12:50:00\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\nΘα χρησιμοποιήσω τα παρακάτω ονόματα στηλών για συντομία.\n\ncolumn_names = ['fire_service', 'forest_department', 'location', 'district',\n                'start_date', 'start_time', 'end_date', 'end_time',\n                'forest', 'forest_area', 'grove', 'grassland', 'marsh', 'crop_fields', 'crop_residues', 'dumps']\n\n\nΠροετοιμασία των ετών 2000-2008\nTα φύλλα από το 2000 έως το 2008 περιέχουν τις παραπάνω στήλες\n\nsheets = data.sheet_names[:9]\nprint(sheets)\n\n['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008']\n\n\nΦορτώνουμε αυτά τα πρώτα 9 φύλλα σε ένα λεξικό με όνομα df_dict\n\ndf_dict = {}\nfor sheet in sheets:\n    df = data.parse(sheet, skiprows=1, names=column_names)\n    df_dict[sheet] = df\n\n\ndf = df_dict['2000']\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 12980 entries, 0 to 12979\nData columns (total 16 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   fire_service       12980 non-null  object        \n 1   forest_department  10037 non-null  object        \n 2   location           12979 non-null  object        \n 3   district           12980 non-null  object        \n 4   start_date         12980 non-null  datetime64[ns]\n 5   start_time         12980 non-null  object        \n 6   end_date           12949 non-null  object        \n 7   end_time           12948 non-null  object        \n 8   forest             12980 non-null  float64       \n 9   forest_area        12980 non-null  float64       \n 10  grove              12980 non-null  float64       \n 11  grassland          12980 non-null  float64       \n 12  marsh              12980 non-null  float64       \n 13  crop_fields        12980 non-null  float64       \n 14  crop_residues      12980 non-null  float64       \n 15  dumps              12980 non-null  float64       \ndtypes: datetime64[ns](1), float64(8), object(7)\nmemory usage: 1.6+ MB\n\n\nΜε εξαίρεση κάποιες τιμές που λείπουν στις στήλες forest_department και location τα υπόλοιπα δεδομένα εμφανίζονται να είναι εντάξει. Τώρα θα επικεντρωθούμε στις στήλες ημερομηνιών και ωρών (start_date, start_time, end_date, end_time).\n\n\nΔιόρθωση των ημερομηνιών\n\n\n\nΛανθασμένες ημερομηνίες\nfor year, df in df_dict.items():\n    if not df[df.end_date.apply(lambda x: isinstance(x, str))].end_date.empty:\n        print(\"Year: \", year)\n        print(df[df.end_date.apply(lambda x: isinstance(x, str))].end_date.sample(2))\n        print()\n\n\nYear:  2000\n4088     2/2/0200\n7599    10/7/0200\nName: end_date, dtype: object\n\nYear:  2001\n5068    28/9/0010\n5259    10/9/0001\nName: end_date, dtype: object\n\nYear:  2002\n7230    19/6/0023\n5856    19/2/0002\nName: end_date, dtype: object\n\nYear:  2004\n7369    13/9/0043\n9407     4/9/0041\nName: end_date, dtype: object\n\nYear:  2006\n2481    31/7/0200\n4788    23/6/0200\nName: end_date, dtype: object\n\nYear:  2007\n1245    28/9/0207\n2108    24/8/0207\nName: end_date, dtype: object\n\nYear:  2008\n3462     11/8/0200\n1407    22/10/0208\nName: end_date, dtype: object\n\n\nΦαίνεται ότι υπάρχουν αρκετές ημερομηνίες που έχουν πληκτρολογηθεί λανθασμένα. Θα προσπαθήσουμε με μία εφαρμογή να διορθώσουμε τις λανθασμένες τιμές.\nΗ εφαρμογή πρώτα ελέγχει ποιες τιμές δεν έχουν μετατραπεί σε datetime objects (strings), και επιστρέφει Timestamp με την σωστή χρονολογία χρησιμοποιώντας τον ίδιο μήνα και ημέρα. Από την διόρθωση θα εξαιρεθούν ημερομηνίες που αφορούν τον Γενάρη του επόμενου έτους μιας και ορισμένες πυρκαγιές ξεκίνησαν Δεκέμβριο και τελείωσαν τον Ιανουάριο του επόμενου έτους.\n\n# Εφαρμογή διόρθωσης του έτους\n\nfrom datetime import datetime\n\ndef fix_year(x, year):\n    \"\"\"A function to replace mistyped year\n    on the end_date column\n    \n    Args:\n        x (str or datetime): value of each row\n        year (int): the correct year to replace\n        \n    Returns:\n        pandas.Timestamp or original value x\n    \"\"\"\n    \n    # records that could not parsed as datetime objects\n    if isinstance(x, str): \n        day, month, _ = x.split('/')\n        return pd.Timestamp(year=year, month=int(month), day=int(day))\n        \n    # check that already parsed dates have the correct year\n    elif isinstance(x, datetime):\n        if x.year == year or pd.isnull(x): # exclude records with the correct year or null value\n            return x\n        elif x.year == (year + 1) and x.month == 1: # exclude records on January of the next year\n            return x \n        else:\n            return pd.Timestamp(year=year, month=int(x.month), day=int(x.day))\n\n’Επειτα χρησιμοποιούμε την εφαρμογή και ενώνουμε ημερ/νία και ώρα σε δύο στήλες που θα ονομάζονται start και end.\n\n# Χρησιμοποιούμε την εφαρμογή\nfor year, df in df_dict.items():\n    df[\"end_date\"] = df.end_date.apply(fix_year, args=[int(year)])\n\n\n# Ενώνουμε ημερομηνία και ώρα\nfor df in df_dict.values():\n    df['start'] = pd.to_datetime(df.start_date.dt.strftime(\"%Y-%m-%d\") + ' ' + df.start_time.map(str))\n    df['end'] = pd.to_datetime(df.end_date.dt.strftime(\"%Y-%m-%d\") + ' ' + df.end_time.map(str), errors='coerce')\n\n\n\nΠροετοιμασία των ετών 2009-2010\nΑπό κάτω παρατηρούμε την προσθήκη της στήλης Δήμος στα δεδομένα σε σχέση με τα προηγούμενα έτη.\n\nprint(data.parse('2009', skiprows=1).columns)\nprint(data.parse('2010', skiprows=1).columns)\n\nIndex(['Υπηρεσία', 'Δασαρχείο', 'Περιοχή - Τοποθεσία', 'Δήμος', 'Νομός',\n       'Ημερ/νία Έναρξης', 'Ώρα Έναρξης', 'Ημερ/νία Κατασβεσης',\n       'Ώρα Κατάσβεσης', 'Δάση', 'Δασική Έκταση', 'Άλση', 'Χορτ/κές Εκτάσεις',\n       'Καλάμια - Βάλτοι', 'Γεωργικές Εκτάσεις', 'Υπολλείματα Καλλιεργειών',\n       'Σκουπιδότοποι'],\n      dtype='object')\nIndex(['Υπηρεσία', 'Δασαρχείο', 'Περιοχή - Τοποθεσία', 'Δήμος', 'Νομός',\n       'Ημερ/νία Έναρξης', 'Ώρα Έναρξης', 'Ημερ/νία Κατασβεσης',\n       'Ώρα Κατάσβεσης', 'Δάση', 'Δασική Έκταση', 'Άλση', 'Χορτ/κές Εκτάσεις',\n       'Καλάμια - Βάλτοι', 'Γεωργικές Εκτάσεις', 'Υπολλείματα Καλλιεργειών',\n       'Σκουπιδότοποι'],\n      dtype='object')\n\n\nΠροσθέτουμε το όνομα borough στη λίστα με τα ονόματα στηλών και φορτώνουμε τα έτη 2009-2010 στο λεξικό μας.\n\ncolumn_names = ['fire_service', 'forest_department', 'location', 'borough', 'district',\n                'start_date', 'start_time', 'end_date', 'end_time',\n                'forest', 'forest_area', 'grove', 'grassland', 'marsh', 'crop_fields', 'crop_residues', 'dumps']\n\n\nsheets = ['2009', '2010']\nfor sheet in sheets:\n    df = data.parse(sheet, skiprows=1, names=column_names)\n    df_dict[sheet] = df\n\nΕλέγχουμε ξανά για λάθος συπληρωμένες ημερομηνίες, τις διορθώνουμε όπως και προηγουμένως και ενώνουμε για την δημιουργία των στηλών start και end.\n\n\n\nΛανθασμένες ημερ/νίες\nfor year, df in df_dict.items():\n    if not df[df.end_date.apply(lambda x: isinstance(x, str))].end_date.empty:\n        print(\"Year: \", year)\n        print(df[df.end_date.apply(lambda x: isinstance(x, str))].end_date.sample(2))\n        print()\n\n\nYear:  2009\n7665     4/9/0209\n5410    17/6/0209\nName: end_date, dtype: object\n\n\n\n# Διόρθωση ημερ/νιών\n\nfor year, df in df_dict.items():\n    if year in sheets:\n        df[\"end_date\"] = df.end_date.apply(fix_year, args=[int(year)])\n\n\n# Ενώνουμε ημερομηνία και ώρα\n\nfor year, df in df_dict.items():\n    if year in sheets:\n        df['start'] = pd.to_datetime(df.start_date.dt.strftime(\"%Y-%m-%d\") + ' ' + df.start_time.map(str))\n        df['end'] = pd.to_datetime(df.end_date.dt.strftime(\"%Y-%m-%d\") + ' ' + df.end_time.map(str), errors='coerce')\n\n\n\nΤο έτος 2011\nΤο 2011 προστίθενται πληροφορίες για το ανθρώπινο δυναμικό και τα οχήματα που συμμετείχαν στην πυρόσβεση.\n\ndata.parse('2011', skiprows=1).columns\n\nIndex(['Υπηρεσία', 'Νομός', 'Ημερ/νία Έναρξης', 'Ώρα Έναρξης',\n       'Ημερ/νία Κατασβεσης', 'Ώρα Κατάσβεσης', 'Δασαρχείο', 'Δήμος',\n       'Περιοχή - Τοποθεσία', 'Δάση', 'Δασική Έκταση', 'Άλση',\n       'Χορτ/κές Εκτάσεις', 'Καλάμια - Βάλτοι', 'Γεωργικές Εκτάσεις',\n       'Υπολλείματα Καλλιεργειών', 'Σκουπιδότοποι', 'ΠΥΡΟΣ. ΣΩΜΑ',\n       'ΠΕΖΟΠΟΡΑ ΤΜΗΜΑΤΑ', 'ΕΘΕΛΟ-ΝΤΕΣ', 'ΣΤΡΑΤΟΣ', 'ΑΛΛΕΣ ΔΥΝΑΜΕΙΣ',\n       'ΠΥΡΟΣ. ΟΧΗΜ.', 'ΟΧΗΜ. ΟΤΑ', 'ΒΥΤΙΟ- ΦΟΡΑ', 'ΜΗΧΑΝΗΜΑΤΑ',\n       'ΕΛΙΚΟ- ΠΤΕΡΑ', 'Α/Φ CL415', 'Α/Φ CL215', 'Α/Φ PZL', 'Α/Φ GRU.'],\n      dtype='object')\n\n\n\n# Στήλες που θα χρησιμοποιηθούν\n\ncolumn_names = ['fire_service', 'district', 'start_date', 'start_time', 'end_date', 'end_time',\n                'forest_department', 'borough', 'location',\n                'forest', 'forest_area', 'grove', 'grassland', 'marsh', 'crop_fields', 'crop_residues', 'dumps',\n                'firemen', 'patrol', 'volunteers', 'military', 'other_groups', 'fire_trucks', 'local_authorities',\n                'private_trucks', 'other_trucks', 'helicopters', 'canadair_new', 'canadair_old', 'pzl',\n                'gru']\n\n\n# Προσθήκη του 2011\n\nsheet = '2011'\ndf = data.parse(sheet, skiprows=1, names=column_names)\ndf_dict[sheet] = df\ndf[\"end_date\"] = df.end_date.apply(fix_year, args=[int(sheet)])\ndf['start'] = pd.to_datetime(df.start_date.dt.strftime(\"%Y-%m-%d\") + ' ' + df.start_time.map(str))\ndf['end'] = pd.to_datetime(df.end_date.dt.strftime(\"%Y-%m-%d\") + ' ' + df.end_time.map(str), errors='coerce')\n\n\n\nΤο έτος 2012\n\n# Το 2012 προστίθεται η διεύθυνση `address`\n\ndata.parse('2012', skiprows=1).columns\n\nIndex(['Υπηρεσία', 'Νομός', 'Ημερ/νία Έναρξης', 'Ώρα Έναρξης',\n       'Ημερ/νία Κατασβεσης', 'Ώρα Κατάσβεσης', 'Δασαρχείο', 'Δήμος',\n       'Περιοχή', 'Διεύθυνση', 'Δάση', 'Δασική Έκταση', 'Άλση',\n       'Χορτ/κές Εκτάσεις', 'Καλάμια - Βάλτοι', 'Γεωργικές Εκτάσεις',\n       'Υπολλείματα Καλλιεργειών', 'Σκουπι-δότοποι', 'ΠΥΡΟΣ. ΣΩΜΑ',\n       'ΠΕΖΟΠΟΡΑ ΤΜΗΜΑΤΑ', 'ΕΘΕΛΟ-ΝΤΕΣ', 'ΣΤΡΑΤΟΣ', 'ΑΛΛΕΣ ΔΥΝΑΜΕΙΣ',\n       'ΠΥΡΟΣ. ΟΧΗΜ.', 'ΟΧΗΜ. ΟΤΑ', 'ΒΥΤΙΟ- ΦΟΡΑ', 'ΜΗΧΑΝΗ-ΜΑΤΑ',\n       'ΕΛΙΚΟ- ΠΤΕΡΑ', 'Α/Φ CL415', 'Α/Φ CL215', 'Α/Φ PZL', 'Α/Φ GRU.'],\n      dtype='object')\n\n\n\n# Ανανέωση των ονομάτων των στηλών\n\ncolumn_names = ['fire_service', 'district', 'start_date', 'start_time', 'end_date', 'end_time',\n                'forest_department', 'borough', 'location', 'address',\n                'forest', 'forest_area', 'grove', 'grassland', 'marsh', 'crop_fields', 'crop_residues', 'dumps',\n                'firemen', 'patrol', 'volunteers', 'military', 'other_groups', 'fire_trucks', 'local_authorities',\n                'private_trucks', 'other_trucks', 'helicopters', 'canadair_new', 'canadair_old', 'pzl',\n                'gru']\n\n\n# Φόρτωση του 2012\n\nsheet = '2012'\ndf = data.parse(sheet, skiprows=1, names=column_names)\ndf_dict[sheet] = df\ndf[\"end_date\"] = df.end_date.apply(fix_year, args=[int(sheet)])\ndf['start'] = pd.to_datetime(df.start_date.dt.strftime(\"%Y-%m-%d\") + ' ' + df.start_time.map(str))\ndf['end'] = pd.to_datetime(df.end_date.dt.strftime(\"%Y-%m-%d\") + ' ' + df.end_time.map(str), errors='coerce')\n\n\n\nΕπεξεργασία των αρχείων για τα έτη 2013-2019\nΕφόσον μέχρι το 2019 δεν έχουμε προσθήκη επιπλέον στηλών, θα αυτοματοποιήσουμε τον τρόπο που διαβάζουμε κάθε αρχείο excel, διορθώνουμε τις ημερ/νίες και το προσθέτουμε στο λεξικό μας.\n\nimport glob\n\nfor sheet in range(2013, 2020):\n    # εύρεση του αρχείου\n    file = glob.glob('datasets/Dasikes_Pyrkagies_' + str(sheet) + '*')[0]\n    \n    # φόρτωση του αρχείου\n    df = pd.read_excel(file, skiprows=1, names=column_names)\n    df_dict[str(sheet)] = df\n    \n    # διόρθωση των ημερομηνιών\n    df[\"end_date\"] = df.end_date.apply(fix_year, args=[sheet])\n    \n    # ένωση των ημερ/νιών με τις ώρες\n    df['start'] = pd.to_datetime(df.start_date.dt.strftime(\"%Y-%m-%d\") + ' ' + df.start_time.map(str))\n    df['end'] = pd.to_datetime(df.end_date.dt.strftime(\"%Y-%m-%d\") + ' ' + df.end_time.map(str), errors='coerce')\n\n\n\nΤα έτη 2020-2021\n\n# Στήλες του 2020 και 2021\n\npd.read_excel('datasets/Dasikes_Pyrkagies_2020.xls', skiprows=1).columns\n\nIndex(['Α/Α ΕΓΓΡΑΦΗΣ', 'Α/Α ENGAGE', 'X-ENGAGE', 'Y-ENGAGE', 'Υπηρεσία',\n       'Νομός', 'Ημερ/νία Έναρξης', 'Ώρα Έναρξης', 'Ημερ/νία Κατασβεσης',\n       'Ώρα Κατάσβεσης', 'Δασαρχείο', 'Δήμος', 'Περιοχή', 'Διεύθυνση', 'Δάση',\n       'Δασική Έκταση', 'Άλση', 'Χορτ/κές Εκτάσεις', 'Καλάμια - Βάλτοι',\n       'Γεωργικές Εκτάσεις', 'Υπολλείματα Καλλιεργειών', 'Σκουπι-δότοποι',\n       'ΠΥΡΟΣ. ΣΩΜΑ', 'ΠΕΖΟΠΟΡΑ ΤΜΗΜΑΤΑ', 'ΕΘΕΛΟ-ΝΤΕΣ', 'ΣΤΡΑΤΟΣ',\n       'ΑΛΛΕΣ ΔΥΝΑΜΕΙΣ', 'ΠΥΡΟΣ. ΟΧΗΜ.', 'ΟΧΗΜ. ΟΤΑ', 'ΒΥΤΙΟ- ΦΟΡΑ',\n       'ΜΗΧΑΝΗ-ΜΑΤΑ', 'ΕΛΙΚΟ- ΠΤΕΡΑ', 'Α/Φ CL415', 'Α/Φ CL215', 'Α/Φ PZL',\n       'Α/Φ GRU.'],\n      dtype='object')\n\n\n\n# Το 2020-2021 προστέθηκαν γεωγρ. συντεταγμένες και ευρετήριο το οποίο δεν θα συμπεριληφθεί στο τελικό αρχείο\n\ncolumn_names = ['longitude', 'latitude', 'fire_service', 'district', 'start_date', 'start_time', 'end_date', 'end_time',\n                'forest_department', 'borough', 'location', 'address',\n                'forest', 'forest_area', 'grove', 'grassland', 'marsh', 'crop_fields', 'crop_residues', 'dumps',\n                'firemen', 'patrol', 'volunteers', 'military', 'other_groups', 'fire_trucks', 'local_authorities',\n                'private_trucks', 'other_trucks', 'helicopters', 'canadair_new', 'canadair_old', 'pzl',\n                'gru']\n\n\n# Φόρτωση των ετών 2020-2021\n\nfor sheet in [2020, 2021]:\n    # εύρεση του αρχείου\n    file = glob.glob('datasets/Dasikes_Pyrkagies_' + str(sheet) + '*')[0]\n    \n    # φόρτωση του αρχείου\n    df = pd.read_excel(file, skiprows=1,\n                   usecols=list(range(2, 36, 1)), names=column_names)\n    df_dict[str(sheet)] = df\n    \n    # διόρθωση των ημερομηνιών\n    df[\"end_date\"] = df.end_date.apply(fix_year, args=[sheet])\n    \n    # ένωση των ημερ/νιών με τις ώρες\n    df['start'] = pd.to_datetime(df.start_date.dt.strftime(\"%Y-%m-%d\") + ' ' + df.start_time.map(str))\n    df['end'] = pd.to_datetime(df.end_date.dt.strftime(\"%Y-%m-%d\") + ' ' + df.end_time.map(str), errors='coerce')\n\nΜπορούμε πλέον να ενώσουμε όλους τους πίνακες από το λεξικό df_dict σε ένα πίνακα με το όνομα wildfires.\n\nwildfires = pd.concat(df_dict.values())\n\nΘέτουμε όποιες ημερ/νίες κατάσβεσης δεν διορθώθηκαν ως κενή τιμή, ελέγχοντας ποιες ημερ/νίες κατάσβεσης προηγούνται των ημερ/νιών έναρξης.\n\nwildfires.loc[(wildfires.start > wildfires.end), 'end'] = pd.NaT\n\nΠαρακάτω είναι το τελικό μας αρχείο.\n\nprint(wildfires.info())\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 223260 entries, 0 to 9513\nData columns (total 36 columns):\n #   Column             Non-Null Count   Dtype         \n---  ------             --------------   -----         \n 0   fire_service       223260 non-null  object        \n 1   forest_department  148424 non-null  object        \n 2   location           154541 non-null  object        \n 3   district           223252 non-null  object        \n 4   start_date         223260 non-null  datetime64[ns]\n 5   start_time         223260 non-null  object        \n 6   end_date           203493 non-null  datetime64[ns]\n 7   end_time           203603 non-null  object        \n 8   forest             223259 non-null  float64       \n 9   forest_area        223260 non-null  float64       \n 10  grove              223260 non-null  float64       \n 11  grassland          223260 non-null  float64       \n 12  marsh              223260 non-null  float64       \n 13  crop_fields        223258 non-null  float64       \n 14  crop_residues      223260 non-null  float64       \n 15  dumps              223258 non-null  float64       \n 16  start              223260 non-null  datetime64[ns]\n 17  end                201425 non-null  datetime64[ns]\n 18  borough            123034 non-null  object        \n 19  firemen            106259 non-null  float64       \n 20  patrol             106305 non-null  float64       \n 21  volunteers         106339 non-null  float64       \n 22  military           106341 non-null  float64       \n 23  other_groups       106335 non-null  float64       \n 24  fire_trucks        106340 non-null  float64       \n 25  local_authorities  106341 non-null  float64       \n 26  private_trucks     106338 non-null  float64       \n 27  other_trucks       106340 non-null  float64       \n 28  helicopters        106339 non-null  float64       \n 29  canadair_new       106341 non-null  float64       \n 30  canadair_old       106340 non-null  float64       \n 31  pzl                106341 non-null  float64       \n 32  gru                106341 non-null  float64       \n 33  address            95057 non-null   object        \n 34  longitude          20309 non-null   float64       \n 35  latitude           20309 non-null   float64       \ndtypes: datetime64[ns](4), float64(24), object(8)\nmemory usage: 63.0+ MB\nNone\n\n\n\nwildfires.sample(5)\n\n\n\n\n\n  \n    \n      \n      fire_service\n      forest_department\n      location\n      district\n      start_date\n      start_time\n      end_date\n      end_time\n      forest\n      forest_area\n      ...\n      private_trucks\n      other_trucks\n      helicopters\n      canadair_new\n      canadair_old\n      pzl\n      gru\n      address\n      longitude\n      latitude\n    \n  \n  \n    \n      11694\n      Π.Υ. ΑΡΓΟΥΣ\n      ΑΡΓΟΛΙΔΑΣ\n      ΑΓ.ΝΙΚΟΛΑΟΣ\n      ΑΡΓΟΛΙΔΟΣ\n      2001-10-03\n      16:19:00\n      2001-10-03\n      16:50:00\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      8088\n      Π.Υ. ΛΑΜΙΑΣ\n      NaN\n      ΛΑΜΙΑ\n      ΦΘΙΩΤΙΔΑΣ\n      2004-10-11\n      12:19:00\n      2004-10-11\n      13:50:00\n      0.0\n      0.1\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1009\n      Π.Υ. ΟΡΕΣΤΙΑΔΑΣ\n      ΔΙΔ/ΧΟΥ\n      ΒΑΛΤΟΣ\n      ΕΒΡΟΥ\n      2019-08-25\n      18:17\n      2019-08-25\n      18:28\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ΑΓΡΟΤΙΚΗ ΠΕΡΙΟΧΗ ΒΑΛΤΟΥ\n      NaN\n      NaN\n    \n    \n      276\n      Π.Υ. ΚΡΩΠΙΑΣ\n      NaN\n      ΚΟΡΟΠΙΟΝ\n      ΑΤΤΙΚΗΣ\n      2003-07-04\n      10:20:00\n      2003-07-04\n      11:15:00\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      7781\n      ΕΘ.Π.Σ. Β ΤΑΞΗΣ ΠΑΡΑΒΟΛΑΣ\n      NaN\n      ΑΓΡΙΝΙΟ\n      ΑΙΤΩΛΟΑΚΑΡΝΑΝΙΑΣ\n      2020-08-27\n      12:35\n      2020-08-27\n      17:25\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ΠΑΝΑΙΤΩΛΙΟ(ΑΜΠΑΡΙΑ)\n      21.4607\n      38.578267\n    \n  \n\n5 rows × 36 columns\n\n\n\n\n# Αποθήκευση του πίνακα στο αρχείο wildfires.csv\nwildfires.to_csv('datasets/wildfires.csv', index=False)"
  },
  {
    "objectID": "portfolio/wildfires/Wildfires in Greece.html#εισαγωγή",
    "href": "portfolio/wildfires/Wildfires in Greece.html#εισαγωγή",
    "title": "Δασικές πυρκαγιές στην Ελλάδα",
    "section": "Εισαγωγή",
    "text": "Εισαγωγή\nImage by Ylvers from Pixabay\nΈνα ακόμη καλοκαίρι έχει φτάσει στο τέλος του. Η Ελλάδα καλείται άλλη μια φορά να αξιολογήσει τις ζημιές που προκάλεσαν οι πυρκαγιές στον φυσικό της πλούτο, τις οικονομικές ζημίες σε καλλιέργειες, και το σημαντικότερο η τραγική απώλεια ανθρώπων και ζώων.\nΣτην ιστοσελίδα του Πυροσβεστικού Σώματος Ελλάδος https://www.fireservice.gr μπορείτε να βρείτε αρχεία δασικών και αστικών συμβάντων στα οποία κλήθηκε να επέμβει το Π.Σ. από το 2000 - 2021 σε μορφή .xls|.xlsx. Για την δημιουργία των γραφημάτων χρειάστηκε να επεξεργαστώ τα αρχεία excel μέσω της python, να επιδιορθώσω κάποιες τιμές και να αποθηκεύσω τα δεδομένα σε ένα αρχείο με όνομα wildfires.csv. Σε αυτή την σελίδα μπορείτε να δείτε με λεπτομέρειες την προεργασία. Παρακάτω ακουλουθεί ένα μικρό δείγμα των δεδομένων.\n\n\nΚώδικας\n# Εισαγωγή των βιβλιοθηκών\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\nsns.set_theme(style='whitegrid', context='talk', palette='colorblind')\n\nwildfires = pd.read_csv('datasets/wildfires.csv', parse_dates=['start', 'end'], low_memory=False)\nwildfires.sample(5)\n\n\n\n\n\n\n  \n    \n      \n      fire_service\n      forest_department\n      location\n      district\n      start_date\n      start_time\n      end_date\n      end_time\n      forest\n      forest_area\n      ...\n      private_trucks\n      other_trucks\n      helicopters\n      canadair_new\n      canadair_old\n      pzl\n      gru\n      address\n      longitude\n      latitude\n    \n  \n  \n    \n      44867\n      Π.Κ. ΚΑΛΛΟΝΗΣ\n      NaN\n      Δ. ΕΡΕΣΣΟΥ\n      ΛΕΣΒΟΥ\n      2003-10-08\n      11:00:00\n      2003-10-08\n      17:30:00\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      69061\n      Π.Υ. ΓΙΑΝΝΙΤΣΩΝ\n      ΕΔΕΣΣΑΣ\n      ΤΣΑΙΝΑΡΛΙ\n      ΠΕΛΛΑΣ\n      2006-09-11\n      12:34:00\n      2006-09-11\n      13:20:00\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      119667\n      Π.Υ. ΠΤΟΛΕΜΑΙΔΑΣ\n      ΚΟΖΑΝΗΣ\n      Τ.Κ. ΚΑΡΥΟΧΩΡΙΟΥ\n      ΚΟΖΑΝΗΣ\n      2011-07-13\n      18:50\n      2011-07-13\n      20:25\n      0.0\n      0.0\n      ...\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      NaN\n      NaN\n      NaN\n    \n    \n      197397\n      Π.Υ. ΠΑΛΑΜΑ\n      ΚΑΡΔΙΤΣΑΣ\n      NaN\n      ΚΑΡΔΙΤΣΑΣ\n      2019-03-21\n      15:05\n      2019-03-21\n      15:51\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ΑΓΡΟΤ.ΔΡΟΜΟΣ ΙΤΕΑΣ-ΣΥΚΕΩΝΑΣ\n      NaN\n      NaN\n    \n    \n      78037\n      Π.Κ. ΣΙΔΗΡΟΚΑΣΤΡΟΥ\n      ΣΙΔΗΡΟΚΑΣΤΡΟΥ\n      ΑΓΡ.ΠΕΡΙΟΧΗ ΓΟΝΙΜΟΥ\n      ΣΕΡΡΩΝ\n      2007-02-05\n      16:50\n      2007-02-05\n      18:05\n      0.0\n      0.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 36 columns\n\n\n\n\nΚλιματολογικά δεδομένα\nΘα χρησιμοποιηθούν δεδομένα των καιρικών συνθηκών της Ελλάδας του εκάστοτε έτους για σύγκριση με τα στοιχεία των πυρκαγιών. Τα δεδομένα προέρχονται από την ιστοσελίδα https://climateknowledgeportal.worldbank.org/ και αφορούν δείκτες παρατεταμένης ξηρασίας και υψηλών θερμοκρασιών ανά έτος (Warm spell duration index, max number of consecutive dry days).\n\n\nΚώδικας\ncol_names = ['year', 'warm_days']\n\nwarm_spell = pd.read_csv('datasets/wsdi_timeseries_annual_era_1970-2020_GRC.csv', usecols=[0, 1], names=col_names, skiprows=2)\n\nwarm_spell.info()\nwarm_spell.head()\n\n\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 51 entries, 0 to 50\nData columns (total 2 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   year       51 non-null     int64  \n 1   warm_days  51 non-null     float64\ndtypes: float64(1), int64(1)\nmemory usage: 944.0 bytes\n\n\n\n\n\n\n  \n    \n      \n      year\n      warm_days\n    \n  \n  \n    \n      0\n      1970\n      11.94\n    \n    \n      1\n      1971\n      1.05\n    \n    \n      2\n      1972\n      0.48\n    \n    \n      3\n      1973\n      3.06\n    \n    \n      4\n      1974\n      1.22\n    \n  \n\n\n\n\n\n\n\n\nΚώδικας\ncol_names = ['year', 'dry_days']\n\nmax_dry_days = pd.read_csv('datasets/cdd_timeseries_annual_era_1970-2020_GRC.csv', usecols=[0, 1], names=col_names, skiprows=2)\n\nprint(max_dry_days.info())\nmax_dry_days.head()\n\n\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 51 entries, 0 to 50\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   year      51 non-null     int64  \n 1   dry_days  51 non-null     float64\ndtypes: float64(1), int64(1)\nmemory usage: 944.0 bytes\nNone\n\n\n\n\n\n\n  \n    \n      \n      year\n      dry_days\n    \n  \n  \n    \n      0\n      1970\n      34.75\n    \n    \n      1\n      1971\n      23.84\n    \n    \n      2\n      1972\n      25.08\n    \n    \n      3\n      1973\n      26.60\n    \n    \n      4\n      1974\n      37.09\n    \n  \n\n\n\n\n\n\n\n\nΕπιλέγουμε δεδομένα μόνο από το 2000 και μετά\nwarm_spell_subset = warm_spell[warm_spell.year >= 2000]\nmax_dry_days_subset = max_dry_days[max_dry_days.year >= 2000]"
  },
  {
    "objectID": "portfolio/wildfires/Wildfires in Greece.html#γραφική-απεικόνιση-των-δεδομένων",
    "href": "portfolio/wildfires/Wildfires in Greece.html#γραφική-απεικόνιση-των-δεδομένων",
    "title": "Δασικές πυρκαγιές στην Ελλάδα",
    "section": "Γραφική απεικόνιση των δεδομένων",
    "text": "Γραφική απεικόνιση των δεδομένων\nΠαρακάτω με την χρήση τον βιβλιοθηκών pandas, matplotlib και seaborn θα παρουσιάσουμε τα δεδομένα με κάποια βασικά γραφήματα.\n\n\nΥπολογισμός περιστατικών ανά έτος\nincidents_per_year = wildfires.start.dt.year.value_counts(sort=False).rename_axis('year').reset_index(name='incidents').merge(warm_spell_subset, on='year').merge(max_dry_days_subset, on='year')\n\n\n\nΑριθμός περιστατικών ανά έτος σε συνδιασμό με κλιματολογικές συνθήκες.\n\n\nΡαβδόγραμμα\nfig, ax = plt.subplots(figsize=(18,8))\nincidents_per_year.plot(kind='bar', ax=ax, x='year', y='incidents')\nincidents_per_year['warm_days'].plot(kind='line', secondary_y=True, ax=ax, color='orangered')\nincidents_per_year['dry_days'].plot(kind='line', secondary_y=True, ax=ax, color='orange')\n\nhandles = ax.get_legend_handles_labels()[0] + ax.right_ax.get_legend_handles_labels()[0]\nlabels = ['Πυρκαγιές', 'Ημέρες καύσωνα', 'Ημέρες ξηρασίας']\nax.legend(handles, labels, loc='upper center', frameon=False, ncol=3, bbox_to_anchor=(0.5, 1.1))\n\nax.grid(False)\nax.right_ax.grid(False)\nax.tick_params(axis='x', labelrotation = 90)\nsns.despine(fig, top=True, left=True)\nax.set_xlabel('')\nax.set_ylabel('# Πυρκαγιών')\nax.right_ax.set_ylabel('Ημέρες');\n\n\n\n\nΠλήθος πυρκαγιών ανά έτος\n\n\n\n\n\nΤα περισσότερα περιστατικά πυρκαγιών καταγράφηκαν το 2000 με 2001 με 12980 και 15303 αντίστοιχα. Καλύτερη χρονιά ήταν το 2014 με 6833 πυρκαγιές. Το πλήθος των πυρκαγιών έχει ξεκάθαρη σχέση με τις ημέρες ξηρασίας που επικρατούν το εκάστοτε έτος ενώ οι ημέρες καύσωνα φαίνεται να έχουν θετική συσχέτιση αλλά μικρότερη.\n\n\nΠαρουσίαση καμμένων εκτάσεων\n\n\nΔιάγραμμα περιοχής\nfig, axes = plt.subplots(3, 1, figsize=(20, 20))\nwildfires.groupby(wildfires.start.dt.year).sum().plot.area(ax=axes[0],\n                                                           y=['forest', 'forest_area', 'grove'],\n                                                           color=['darkgreen', 'forestgreen', 'yellowgreen'],\n                                                           linewidth=0)\n\nwildfires.groupby(wildfires.start.dt.year).sum().plot.area(ax=axes[1],\n                                                           y=['crop_fields', 'crop_residues'],\n                                                           color=['gold', 'darkgoldenrod'],\n                                                           linewidth=0)\n\nwildfires.groupby(wildfires.start.dt.year).sum().plot.area(ax=axes[2],\n                                                           y=['grassland', 'marsh', 'dumps'],\n                                                           color=['lightgreen', 'olive', 'silver'],\n                                                           linewidth=0)\n\n# make a square root scale for y, otherwise should recalculate area field\ndef forward(x):\n    return x**(1/2)\n\n\ndef inverse(x):\n    return x**2\n\nmylabels = [['Δάσος', 'Δασική έκταση', 'Άλση'], ['Καλλιέργειες', 'Υπολείμματα καλλιεργειών'], ['Χορτ/κές Εκτάσεις', 'Καλάμια - Βάλτοι', 'Σκουπιδότοποι']]\n\nfor i, ax in enumerate(axes):\n    ax.xaxis.grid(False)\n    sns.despine(ax=ax, right=True, left=True, top=True)\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(2))\n    ax.set_ylim([0, 1_500_000])\n    ax.set_yscale('function', functions=(forward, inverse)) # set the square root scale\n    ax.yaxis.set_major_locator(ticker.FixedLocator([0, 10_000, 50_000, 100_000, 200_000, 400_000,\n                                                    600_000, 800_000, 1_100_000, 1_400_000]))\n    ax.yaxis.get_major_formatter().set_scientific(False)\n    ax.set_xlabel('')\n    ax.set_ylabel('Καμμένη έκταση (στρέμματα)')\n    handles, _ = ax.get_legend_handles_labels()\n    ax.legend(handles, mylabels[i], loc='upper center', frameon=False, ncol=3, bbox_to_anchor=(0.5, 1.1))\n\n\n\n\nΚαμμένες εκτάσεις ανά έτος (2000 - 2021)\n\n\n\n\n\nΤο 2000 είχαμε την πυρκαγιά της Σάμου, τη μεγαλύτερη στην ιστορία του νησιού με 145.000 στρέμματα καμμένα όπως επίσης και του Ξυλόκαστρου Κορινθίας όπου κάηκαν συνολικά 200.000 στρέμματα.1\nΤο 2007, η χειρότερη χρονιά για την Πελοπόννησο, με την μεγαλύτερη καταστροφή φυσικού πλούτου μέχρι σήμερα. Σε δύο μήνες, Ιούλιο και Αύγουστο ξέσπασαν πυρκαγιές στο Αίγιο, την Καλαμάτα, την Καλλιθέα Λακωνίας, την Ζαχάρω και το Αγραπιδώρι Ηλείας όπου έκαψαν άνω του ενός εκατομμυρίου στρεμμάτων.2\nΤο 2021 ήταν σειρά της Εύβοιας να υποστεί μία από τις μεγαλύτερες καταστροφές του φυσικού της πλούτου με 511.854 καμμένα στρέμματα. Ακολούθησε η πυρκαγιά στην Αρχαία Ολυμπία με 150.000 καμμένα στρέμματα και την Ανατολική Μάνη με 101.001 καμμένα στρέμματα.3\n\n\nΔιάρκεια πυρκαγιών\n’Επειτα θα υπολογίσουμε την διάρκεια των πυρκαγιών σε όσα δεδομένα έχουμε διαθέσιμη την ημερ/νία πυρόσβεσης.\n\n\nΥπολογισμός διάρκειας πυρκαγιών σε όσα δεδομένα έχουμε διαθέσιμη την ημερ/νία πυρόσβεσης\n# αφαιρούμε τις τιμές που λείπουν\nwildfires_subset = wildfires[['start', 'end']].dropna()\n\nprint(\"Πλήθος εγγραφών για τις οποίες υπολογίσαμε την διάρκεια: \", wildfires_subset.shape[0])\n\nwildfires_subset['duration'] = (wildfires_subset.end - wildfires_subset.start).dt.total_seconds() / 360 / 24\n\n\nΠλήθος εγγραφών για τις οποίες υπολογίσαμε την διάρκεια:  201425\n\n\n\n\nΓραμμικό γράφημα\nfig, ax = plt.subplots(figsize=(14, 8))\nwildfires_subset.groupby(wildfires_subset.start.dt.year).mean().plot(y='duration', ax=ax, legend=False)\n\nsns.despine(ax=ax, top=True, left=True, right=True, bottom=True)\nax.xaxis.grid(False)\nax.xaxis.set_major_locator(ticker.MultipleLocator(2))\nax.set_ylim([0, 6])\nax.set(xlabel='', ylabel='Διάρκεια σε ημέρες');\n\n\n\n\nΕτήσιος μέσος όρος διάρκειας πυρκαγιών (2000 - 2021)\n\n\n\n\n\nΜε την πάροδο των ετών η διάρκεια των πυρκαγιών αυξάνεται με αποκορύφωμα το 2021. Επίσης με μία σύγκριση με το προηγούμενο γράφημα υπάρχει σαφής συσχέτιση της διάρκειας με την έκταση των καταστροφών.\n\n\nΠαρουσίαση του έμψυχου δυναμικού πυρόσβεσης και των οχημάτων\n\n\nΕπιλογή των ετών 2011-2021 για την παρουσίαση του ανθρώπινου δυναμικού και των οχημάτων πυρόσβεσης\nwildfires_subset = wildfires[wildfires.start.dt.year >= 2011]\n\n\n\n\nΥπολογισμός του μέσου όρου πυροσβεστών ανά περιστατικό\npersonnel = wildfires_subset.groupby(wildfires_subset.start.dt.year).mean()[['firemen',\n                                                                             'patrol',\n                                                                             'volunteers',\n                                                                             'military',\n                                                                             'other_groups']]\n\n\n\n\nΡαβδόγραμμα\nfig, ax = plt.subplots(figsize=(14, 10))\npersonnel.plot.bar(stacked=True, ax=ax)\nsns.despine(ax=ax, top=True, left=True, right=True)\n\nax.xaxis.grid(False)\nax.set_ylim([0, 9])\n\nhandles, _ = ax.get_legend_handles_labels()\nlabels = ['Πυροσβεστικό Σώμα', 'Πεζοπόρα τμήματα', 'Εθελοντές', 'Στρατός', \"'Αλλες δυνάμεις\"]\nax.legend(handles, labels, loc='right', bbox_to_anchor=(1.3,0.5), frameon=False)\nax.set(xlabel='', ylabel='Πυροσβέστες ανά περιστατικό');\n\n\n\n\nΜέσος όρος πυροσβεστών ανά περιστατικό ετησίος (2011-2021)\n\n\n\n\n\nΟ μέσος όρος των πυροσβεστών που επιχειρούν ανά περιστατικό ετησίως είναι σχετικά σταθερός. Ακόμη και το 2012 και 2021 που επιχείρησαν κατά μέσο όρο περισσότεροι πυροσβέστες οι καταστροφές ήταν εκτεταμένες όπως είδαμε από τα προηγούμενα γραφήματα.\n\n\nΥπολογισμός του μέσου όρου οχημάτων ανά περιστατικό\nvehicles = wildfires_subset.groupby(wildfires_subset.start.dt.year).mean()[\n                                                                       ['fire_trucks',\n                                                                        'local_authorities',\n                                                                        'private_trucks',\n                                                                        'other_trucks',\n                                                                        'helicopters',\n                                                                        'canadair_new',\n                                                                        'canadair_old',\n                                                                        'pzl',\n                                                                        'gru']\n                                                                            ]\n\n\n\n\nΡαβδόγραμμα\nfig, ax = plt.subplots(figsize=(14, 10))\nvehicles.plot.bar(stacked=True, ax=ax)\n\nax.xaxis.grid(False)\nsns.despine(ax=ax, top=True, left=True, right=True)\n\nhandles, _ = ax.get_legend_handles_labels()\nlabels = ['Πυροσ. Οχήματα', 'Οχήματα ΟΤΑ', 'Βυτιοφόρα', 'Μηχανήματα', 'Ελικόπτερα', 'Canadair CL415', 'Canadair CL215' 'PZL', 'GRU']\nax.legend(handles, labels, loc='right', bbox_to_anchor=(1.3,0.5), frameon=False)\nax.set_xlabel('');\n\n\n\n\nΜέσος όρος οχημάτων ανά περιστατικό ετησίος (2011-2021)\n\n\n\n\n\nΑκριβώς ίδια είναι η εικόνα από το μέσο όρο οχημάτων που χρησιμοποιήθηκαν στην πυρόσβεση ανά έτος.\n\n\nΣυμπεράσματα\nΔυστυχώς παρόλες τις προσπάθειες πυρόσβεσης δεν εμφανίζεται κάποια βελτίωση στην ετήσια αποτίμηση των ζημιών από πυρκαγιές. Αν συνυπολογίσουμε ότι οι καιρικές συνθήκες όπως ο καύσωνας και η ξηρασία επιδεινώνουν την κατάσταση και λόγω της κλιματικής αλλαγής τέτοιες συνθήκες θα είναι συχνότερες, πρέπει να αναμένουμε ότι τα επόμενα χρόνια η συχνότητα, η διάρκεια και η έκταση των ζημιών θα αυξάνονται. Ανάλογα λοιπόν, θα πρέπει να αυξήσουμε την προσπάθεια πυρόσβεσης ποιοτικά και ποσοτικά, όπως επίσης και τις προσπάθεις πρόληψης, αν θέλουμε μελλοντικά να διατηρήσουμε ή ακόμη και να μειώσουμε το ετήσιο κόστος ζημιών από πυρκαγιές."
  },
  {
    "objectID": "portfolio/filmpit/filmpit-scraping.html",
    "href": "portfolio/filmpit/filmpit-scraping.html",
    "title": "Οι ταινίες του Film Pit σε γραφήματα",
    "section": "",
    "text": "To Film Pit είναι ένα podcast το οποίο διανύει τον τέταρτο χρόνο προβολής του. Ασχολείται με την κριτική ταινιών μικρής δημοφιλίας και προϋπολογισμού κυρίως από την δεκαετία του 80 και 90. Στο διάστημα αυτό οι 3 δημιουργοί του έχουν παρουσιάσει και σχολιάσει 114 ταινίες. Όντας ακροατής, θέλησα να βρω περισσότερες πληροφορίες για τις ταινίες που επέλεξαν.\nΧρησιμοποίησα βιβλιοθήκες απο την γλώσσα Python για να συγκεντρώσω πληροφορίες από τις ιστοσελίδες Internet Movie Database (IMDB) και The Movie Database (TMDB). Οι βιβλιοθήκες που χρησιμοποιήθηκαν είναι οι requests, BeautifulSoup, pandas και numpy. Σε αυτή την σελίδα θα βρείτε λεπτομερείς οδηγίες για την συλλογή των πληροφοριών.\nΤα δεδομένα παρουσιάζονται υπό μορφή γραφημάτων με την βοήθεια του προγράμματος Tableau και αναρτήθηκαν στην πλατφόρμα Tableau public. Τα γραφήματα που δημιουργούνται μπορούν να συνδιαστούν σε dashboards και να παρουσιαστούν υπό την μορφή stories, αναρτημένα στην υπηρεσία Τableau Public. Όπως βλέπετε παρακάτω το αποτέλεσμα μπορεί να ενσωματωθεί σε οποιαδήποτε ιστοσελίδα επιθυμείτε."
  },
  {
    "objectID": "portfolio/filmpit/helper.html",
    "href": "portfolio/filmpit/helper.html",
    "title": "Οι ταινίες του Film Pit σε γραφήματα",
    "section": "",
    "text": "Αρχικά με τις βιβλιοθήκες requests και BeautifulSoup συγκέντρωσα όλους τους τίτλους των ταινιών από την ιστοσελίδα του Film Pit και τις αποθήκευσα σε ένα αρχείο με όνομα movie_titles.csv.\n\n# Φόρτωση των απαραίτητων βιβλιοθηκών\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\nimport pandas as pd\nimport numpy as np\nfrom os import path\n\n\n# Ελέγχουμε έαν υπάρχει ήδη το αρχείο αλλιώς το δημιουργούμε\nif path.exists(path.join('datasets','movie_titles.csv')):\n    movie_titles = pd.read_csv(path.join('datasets', 'movie_titles.csv')).squeeze('columns')\nelse:\n    URL = \"https://thefilmpit.com\"\n\n    def get_movies_title(URL):\n        \"\"\"Scrapes podcasts hrefs to get movie titles\"\"\"\n        movie_titles = []\n        page = requests.get(URL)\n        next_link = None   \n        \n        if page.ok:\n            soup = BeautifulSoup(page.content, 'html.parser')\n            try:\n                # Ελέγχουμε αν υπάρχει επόμενη σελίδα στο site\n                next_link = soup.find('link', {'rel':'next',}).get('href')\n            except AttributeError:\n                print(\"This page hasn't a next link\")\n\n            # Απο τους συνδέσμους href λαμβάνουμε τους τίτλους\n            for podcast in soup.find_all('a', attrs={'rel':'bookmark'}): \n                podcast_link = podcast.get('href')\n                title = podcast_link.split('/')[-2].replace('-', ' ')\n                movie_titles.append(title)\n        return movie_titles, next_link\n\n    next_link = URL\n    movie_titles = []\n\n    while next_link:\n        print(f\"Scraping next link: {next_link}\")\n        titles, next_link = get_movies_title(next_link)\n        movie_titles += titles\n        time.sleep(5)\n    print(\"Scraping finished.\")\n\nScraping next link: https://thefilmpit.com\nScraping next link: https://thefilmpit.com/page/2/\nScraping next link: https://thefilmpit.com/page/3/\nScraping next link: https://thefilmpit.com/page/4/\nScraping next link: https://thefilmpit.com/page/5/\nScraping next link: https://thefilmpit.com/page/6/\nScraping next link: https://thefilmpit.com/page/7/\nScraping next link: https://thefilmpit.com/page/8/\nScraping next link: https://thefilmpit.com/page/9/\nScraping next link: https://thefilmpit.com/page/10/\nScraping next link: https://thefilmpit.com/page/11/\nScraping next link: https://thefilmpit.com/page/12/\nThis page hasn't a next link\nScraping finished.\n\n\nΚάποιοι τίτλοι χρειάζονταν διόρθωση όπως βλέπετε παρακάτω.\n\nif not path.exists(path.join('datasets','movie_titles.csv')):\n    movie_titles += ['never too young to die', 'Dr Caligari', 'Yeti'] # 3 ταινίες δεν υπάρχουν στην ιστοσελίδα\n    movie_titles = pd.Series(movie_titles)\n    \n    fixed_titles = {'diagalaxiaki poiotita galaxy of terror': 'galaxy of terror',\n                    'i scholi tou gkontfrei sakura killers': 'sakura killers',\n                    'exairetika petsino podkast the punisher': 'the punisher',\n                    'o rambu tis indonisias einai edo': 'Rambu aka The Intruder',\n                    'tha einai san star gouorz alla den tha einai star gouorz battle beyond the stars': 'battle beyond the stars',\n                    'brady idrotas kai pioti tie night stalker': 'night stalker',\n                    'pao na kano penintarika double dragon': 'double dragon',\n                    'mousikorama shock em dead': 'shock em dead',\n                    'asiatiki tourne 3 undefeatable': 'undefeatable',\n                    'asiatiki tourne 2 w is war': 'w is war',\n                    'asiatiki tourne 1 for your height only': 'for your height only',\n                    'ena mikro mousiko breik rappin': 'rappin',\n                    'dark night of the scarecrow feat elina dimitriadi': 'dark night of the scarecrow',\n                    'prom night feat elina dimitriadi': 'prom night',\n                    'aerobicide': 'Killer Workout',\n                    'an american hippie in paris': 'An American Hippie in Israel',\n                    'tc2000': 'tc 2000',\n                    'american ninia': 'american ninja',\n                    'class of nukem high': \"Class of Nuke 'Em High\",\n                    'superman 4': \"superman iv\"\n                    }\n\n    movie_titles.replace(fixed_titles, inplace=True)\n    movie_titles.drop([91, 109], inplace=True)\n\n\n# Αποθήκευση των τίτλων στο αρχείο movie_title.csv\n\nif not path.exists(path.join('datasets','movie_titles.csv')):\n    movie_titles.to_csv(path.join('datasets','movie_titles.csv'), index=False, header=['titles'])"
  },
  {
    "objectID": "portfolio/filmpit/helper.html#εξαγωγή-πληροφοριών-από-την-ιστοσελίδα-the-movie-database",
    "href": "portfolio/filmpit/helper.html#εξαγωγή-πληροφοριών-από-την-ιστοσελίδα-the-movie-database",
    "title": "Οι ταινίες του Film Pit σε γραφήματα",
    "section": "Εξαγωγή πληροφοριών από την ιστοσελίδα The Movie Database",
    "text": "Εξαγωγή πληροφοριών από την ιστοσελίδα The Movie Database\nΓια την εξαγωγή πληροφοριών από το TMDB χρησιμοποίησα την βιβλιοθήκη tmdbv3api.\n\nif path.exists(path.join('datasets','movies_tmdb.csv')):\n    movies_tmdb = pd.read_csv(path.join('datasets','movies_tmdb.csv'), index_col=0)\nelse:\n    from tmdbv3api import TMDb\n    from config import config\n    tmdb = TMDb()\n    tmdb.api_key = config['tmdb_api_key'] # το api key δίνεται δωρεάν με απλή εγγραφή στο TMDB\n\n    from tmdbv3api import Movie\n    movie = Movie()\n\n    basic_info = {}\n    not_found = []\n    for title in movie_titles.to_list():\n        print('Fetching ' + title)\n        try:\n            # Χρησιμοποιούμε το πρώτο αποτέλεσμα αναζήτησης\n            basic_info[title] = movie.search(title)[0] \n        except IndexError:\n            # Αποθηκεύουμε τους τίτλους που δεν βρέθηκαν στην λίστα not_found\n            not_found.append(title) \n\n    print('Movies not found: ', not_found)\n\nFetching broken america the killing of america\nFetching broken america star time\nFetching edge of honour\nFetching mind trap aka danger usa\nFetching caged fury\nFetching silk\nFetching raw nerve\nFetching raw justice\nFetching the baby\nFetching dont go in the house\nFetching conquest\nFetching battletruck\nFetching ninja squad\nFetching ultimax force\nFetching punk vacation\nFetching suburbia\nFetching An American Hippie in Israel\nFetching blood beach\nFetching hunters of the golden cobra\nFetching intent to kill\nFetching maniac cop\nFetching hard ticket to hawaii\nFetching warlock\nFetching maximum overdrive\nFetching nekromantik\nFetching manos\nFetching superman iv\nFetching flash gordon\nFetching surf nazis must die\nFetching to kako\nFetching zardoz\nFetching dark angel\nFetching girl in room 2a\nFetching tc 2000\nFetching interzone\nFetching zaat\nFetching american ninja\nFetching krull\nFetching beastmaster\nFetching raiders of the living dead\nFetching 315\nFetching q winged serpent\nFetching karate warrior 6\nFetching savage streets\nFetching amsterdamned\nFetching fatal deviation\nFetching black gestapo\nFetching ss experiment love camp\nFetching ilsa she wolf of the ss\nFetching yor\nFetching savage justice\nFetching commander\nFetching nightstick\nFetching captain america\nFetching stone cold\nFetching programmed to kill\nFetching rotor\nFetching Killer Workout\nFetching russian ninja\nFetching new years evil\nFetching star wars holiday special\nFetching christmas evil\nFetching little hercules\nFetching death wish 3\nFetching chopping mall\nFetching savage beach\nFetching mad foxes\nFetching gymkata\nFetching galaxy of terror\nFetching mac and me\nFetching sakura killers\nFetching the punisher\nFetching Rambu aka The Intruder\nFetching battle beyond the stars\nFetching night stalker\nFetching double dragon\nFetching shock em dead\nFetching undefeatable\nFetching w is war\nFetching for your height only\nFetching ninja silent assassin\nFetching samurai cop\nFetching lambada set the night on fire\nFetching double trouble\nFetching rappin\nFetching the perfect weapon\nFetching deadly prey\nFetching the rage\nFetching warbirds\nFetching starcrash\nFetching arena\nFetching galaxina\nFetching star slammer\nFetching jack frost\nFetching santa with muscles\nFetching elves\nFetching the terrible infocast\nFetching neon maniacs\nFetching dark night of the scarecrow\nFetching prom night\nFetching cheerleader camp\nFetching tuff turf\nFetching Class of Nuke 'Em High\nFetching class of 1984\nFetching exterminators of the year 3000\nFetching 1990 bronx warriors\nFetching endgame\nFetching 2019 after the fall of new york\nFetching hands of steel\nFetching bloodsport\nFetching no retreat no surrender\nFetching thunder\nFetching miami connection\nFetching an intermission with michael pare\nFetching robowar\nFetching frankenhooker\nFetching ninja terminator\nFetching never too young to die\nFetching Dr Caligari\nFetching Yeti\nMovies not found:  ['broken america the killing of america', 'broken america star time', 'edge of honour', 'mind trap aka danger usa', 'Rambu aka The Intruder', 'lambada set the night on fire', 'the terrible infocast', 'an intermission with michael pare']\n\n\n\n# Ελέγχουμε ότι το πρώτο αποτέλεσμα αναζήτησης ήταν το σωστό\nif not path.exists(path.join('datasets','movies_tmdb.csv')):\n    for title in movie_titles.to_list():\n        if basic_info.get(title):\n            print(title, ': ', basic_info[title]['title'], basic_info[title]['release_date'][:4])\n\ncaged fury :  Caged Fury 1990\nsilk :  Silk Road 2021\nraw nerve :  Raw Nerve 1991\nraw justice :  Raw Justice 1994\nthe baby :  The Boss Baby: Family Business 2021\ndont go in the house :  Don't Go in the House 1979\nconquest :  Game of Thrones - Conquest & Rebellion: An Animated History of the Seven Kingdoms 2017\nbattletruck :  Warlords of the 21st Century 1982\nninja squad :  The Ninja Squad 1986\nultimax force :  Ultimax Force 1987\npunk vacation :  Punk Vacation 1990\nsuburbia :  Infidelity in Suburbia 2017\nAn American Hippie in Israel :  An American Hippie in Israel 1972\nblood beach :  Blood Beach 1980\nhunters of the golden cobra :  The Hunters of the Golden Cobra 1982\nintent to kill :  Intent to Kill 1992\nmaniac cop :  Maniac Cop 1988\nhard ticket to hawaii :  Hard Ticket to Hawaii 1987\nwarlock :  Warlock: The Armageddon 1993\nmaximum overdrive :  Maximum Overdrive 1986\nnekromantik :  Nekromantik 1987\nmanos :  Manos: The Hands of Fate 1966\nsuperman iv :  Superman IV: The Quest for Peace 1987\nflash gordon :  Flash Gordon 1980\nsurf nazis must die :  Surf Nazis Must Die 1987\nto kako :  Evil 2005\nzardoz :  Zardoz 1974\ndark angel :  Dark Angel 1990\ngirl in room 2a :  The Girl in Room 2A 1974\ntc 2000 :  TC 2000 1993\ninterzone :  Interzone 1987\nzaat :  Zaat 1971\namerican ninja :  American Ninja 1985\nkrull :  Krull 1983\nbeastmaster :  The Beastmaster 1982\nraiders of the living dead :  Raiders of the Living Dead 1986\n315 :  3:15 1986\nq winged serpent :  Q 1982\nkarate warrior 6 :  Karate Warrior 6 1993\nsavage streets :  Savage Streets 1984\namsterdamned :  Amsterdamned 1988\nfatal deviation :  Fatal Deviation 1998\nblack gestapo :  The Black Gestapo 1975\nss experiment love camp :  SS Experiment Love Camp 1976\nilsa she wolf of the ss :  Ilsa: She Wolf of the SS 1975\nyor :  Yor, the Hunter from the Future 1983\nsavage justice :  Savage Justice 1988\ncommander :  Wing Commander 1999\nnightstick :  Nightstick 1987\ncaptain america :  Captain America: Civil War 2016\nstone cold :  Stone Cold 2005\nprogrammed to kill :  Programmed to Kill 1987\nrotor :  R.O.T.O.R. 1987\nKiller Workout :  Killer Workout 1987\nrussian ninja :  The Russian Ninja 1989\nnew years evil :  New Year's Evil 1980\nstar wars holiday special :  LEGO Star Wars Holiday Special 2020\nchristmas evil :  Christmas Evil 1980\nlittle hercules :  Little Hercules 2009\ndeath wish 3 :  Death Wish 3 1985\nchopping mall :  Chopping Mall 1986\nsavage beach :  L.E.T.H.A.L. Ladies: Return to Savage Beach 1998\nmad foxes :  Mad Foxes 1981\ngymkata :  Gymkata 1985\ngalaxy of terror :  Galaxy of Terror 1981\nmac and me :  Mac and Me 1988\nsakura killers :  Sakura Killers 1987\nthe punisher :  The Punisher 2004\nbattle beyond the stars :  Battle Beyond the Stars 1980\nnight stalker :  The Night Stalker 1972\ndouble dragon :  Double Dragon 1994\nshock em dead :  Shock 'Em Dead 1991\nundefeatable :  Undefeatable 1993\nw is war :  W Is War 1983\nfor your height only :  For Y'ur Height Only 1981\nninja silent assassin :  Ninja Silent Assassin 1987\nsamurai cop :  Samurai Cop 1991\ndouble trouble :  Double Trouble 1992\nrappin :  Rappin' 1985\nthe perfect weapon :  The Perfect Weapon 2016\ndeadly prey :  Deadly Prey 1987\nthe rage :  The Rage: Carrie 2 1999\nwarbirds :  Warbirds 2008\nstarcrash :  Starcrash 1978\narena :  Arena 2011\ngalaxina :  Galaxina 1980\nstar slammer :  Star Slammer 1986\njack frost :  Jack Frost 1997\nsanta with muscles :  Santa with Muscles 1996\nelves :  Elves 1989\nneon maniacs :  Neon Maniacs 1986\ndark night of the scarecrow :  Dark Night of the Scarecrow 1981\nprom night :  Prom Night 1980\ncheerleader camp :  #1 Cheerleader Camp 2010\ntuff turf :  Tuff Turf 1985\nClass of Nuke 'Em High :  Class of Nuke 'Em High 1986\nclass of 1984 :  Class of 1984 1982\nexterminators of the year 3000 :  Exterminators of the Year 3000 1983\n1990 bronx warriors :  1990: The Bronx Warriors 1982\nendgame :  Avengers: Endgame 2019\n2019 after the fall of new york :  2019: After the Fall of New York 1983\nhands of steel :  Hands of Steel 1986\nbloodsport :  Bloodsport 1988\nno retreat no surrender :  No Retreat, No Surrender 1986\nthunder :  Thor: Love and Thunder 2022\nmiami connection :  Miami Connection 1987\nrobowar :  Robowar 1988\nfrankenhooker :  Frankenhooker 1990\nninja terminator :  Ninja Terminator 1985\nnever too young to die :  Never Too Young to Die 1986\nDr Caligari :  The Cabinet of Dr. Caligari 1920\nYeti :  Mission Kathmandu: The Adventures of Nelly & Simon 2017\n\n\nΔυστυχώς τα πρώτο αποτέλεσμα από κάθε ταινία δεν είναι πάντα το επιθυμητό δεδομένου ότι πολλές ταινίες έχουν τον ίδιο ή παρόμοιο τίτλο. Παρακάτω επιλέγουμε τις σωστές ταινίες μέσω του αριθμού id κάθε ταινίας και προσθέτουμε και όσες δεν βρέθηκαν στην αναζήτηση.\n\nif not path.exists(path.join('datasets','movies_tmdb.csv')):\n    \n    \n    wrong_matches = {'Rambu aka The Intruder': '81944',\n                     'lambada set the night on fire': '117269',\n                     'the baby': '28156',\n                     'conquest': '27232',\n                     'suburbia': '28054',\n                     'warlock': '11342',\n                     'to kako': '39897',\n                     'commander': '205697',\n                     'captain america': '13995',\n                     'star wars holiday special': '74849',\n                     'the punisher': '8867',\n                     'night stalker':'66474',\n                     'the perfect weapon': '34421',\n                     'the rage': '114936',\n                     'warbirds': '219359',\n                     'arena': '44796',\n                     'jack frost': '27318',\n                     'elves': '30452',\n                     'prom night': '36599',\n                     'cheerleader camp': '40087',\n                     'endgame': '28850',\n                     'thunder': '109104',\n                     'Dr Caligari': '35642',\n                     'Yeti': '92316'}\n    \n\n    movie_ids = {title:info['id'] for title, info in basic_info.items()}\n    movie_ids.update(wrong_matches)\n\nΕφόσον πλέον έχουμε το σωστό id για όλες τις ταινίες εξάγουμε όλες τις πληροφορίες που χρειαζόμαστε, τις αποθηκεύουμε στο λεξικό movie_records και με την βιβλιοθήκη pandas δημιουργούμε το αρχείο movies_tmdb.csv.\n\nif not path.exists(path.join('datasets','movies_tmdb.csv')):\n    print('Starting api request')\n    movie_records = {}\n    for title, id in movie_ids.items():\n        \n        print(f\"Fetching {title}\")\n        mov = movie.details(id)\n        temp_list = [\n            mov.imdb_id,\n            mov.original_title,\n            mov.budget,\n            mov.revenue,\n            mov.runtime,\n            mov.popularity,\n            [company['name'] for company in mov.production_companies],\n            [key['name'] for key in mov.keywords.keywords],\n            [act['name'] for act in mov.casts.cast],\n            mov.overview\n        ]\n        movie_records[mov.title] = temp_list\n    \n    print('TMDB request finished')\n    columns = ['imdb_id',\n               'original_title',\n               'budget',\n               'revenue',\n               'runtime',\n               'popularity',\n               'production_companies',\n               'keywords',\n               'cast',\n               'overview'\n              ]\n    movies_tmdb = pd.DataFrame.from_dict(movie_records, orient='index', columns=columns)\n\nStarting api request\nFetching caged fury\nFetching silk\nFetching raw nerve\nFetching raw justice\nFetching the baby\nFetching dont go in the house\nFetching conquest\nFetching battletruck\nFetching ninja squad\nFetching ultimax force\nFetching punk vacation\nFetching suburbia\nFetching An American Hippie in Israel\nFetching blood beach\nFetching hunters of the golden cobra\nFetching intent to kill\nFetching maniac cop\nFetching hard ticket to hawaii\nFetching warlock\nFetching maximum overdrive\nFetching nekromantik\nFetching manos\nFetching superman iv\nFetching flash gordon\nFetching surf nazis must die\nFetching to kako\nFetching zardoz\nFetching dark angel\nFetching girl in room 2a\nFetching tc 2000\nFetching interzone\nFetching zaat\nFetching american ninja\nFetching krull\nFetching beastmaster\nFetching raiders of the living dead\nFetching 315\nFetching q winged serpent\nFetching karate warrior 6\nFetching savage streets\nFetching amsterdamned\nFetching fatal deviation\nFetching black gestapo\nFetching ss experiment love camp\nFetching ilsa she wolf of the ss\nFetching yor\nFetching savage justice\nFetching commander\nFetching nightstick\nFetching captain america\nFetching stone cold\nFetching programmed to kill\nFetching rotor\nFetching Killer Workout\nFetching russian ninja\nFetching new years evil\nFetching star wars holiday special\nFetching christmas evil\nFetching little hercules\nFetching death wish 3\nFetching chopping mall\nFetching savage beach\nFetching mad foxes\nFetching gymkata\nFetching galaxy of terror\nFetching mac and me\nFetching sakura killers\nFetching the punisher\nFetching battle beyond the stars\nFetching night stalker\nFetching double dragon\nFetching shock em dead\nFetching undefeatable\nFetching w is war\nFetching for your height only\nFetching ninja silent assassin\nFetching samurai cop\nFetching double trouble\nFetching rappin\nFetching the perfect weapon\nFetching deadly prey\nFetching the rage\nFetching warbirds\nFetching starcrash\nFetching arena\nFetching galaxina\nFetching star slammer\nFetching jack frost\nFetching santa with muscles\nFetching elves\nFetching neon maniacs\nFetching dark night of the scarecrow\nFetching prom night\nFetching cheerleader camp\nFetching tuff turf\nFetching Class of Nuke 'Em High\nFetching class of 1984\nFetching exterminators of the year 3000\nFetching 1990 bronx warriors\nFetching endgame\nFetching 2019 after the fall of new york\nFetching hands of steel\nFetching bloodsport\nFetching no retreat no surrender\nFetching thunder\nFetching miami connection\nFetching robowar\nFetching frankenhooker\nFetching ninja terminator\nFetching never too young to die\nFetching Dr Caligari\nFetching Yeti\nFetching Rambu aka The Intruder\nFetching lambada set the night on fire\nTMDB request finished\n\n\n\n# Αντικαθιστούμε το 0 ως τιμή που λείπει\nmovies_tmdb[['budget', 'revenue', 'runtime']] = movies_tmdb[['budget', 'revenue', 'runtime']].replace({0: np.nan})\n\nΠαρακάτω είναι ένα μικρό δείγμα του αρχείου.\n\nmovies_tmdb.sample(5)\n\n\n\n\n\n  \n    \n      \n      imdb_id\n      original_title\n      budget\n      revenue\n      runtime\n      popularity\n      production_companies\n      keywords\n      cast\n      overview\n    \n  \n  \n    \n      Blood Beach\n      tt0082083\n      Blood Beach\n      2000000.0\n      NaN\n      92.0\n      8.242\n      [Compass International Pictures, Empress Film ...\n      [beach, monster, sand, spoof]\n      [David Huffman, Marianna Hill, Burt Young, Joh...\n      Something or someone is attacking people one b...\n    \n    \n      Elves\n      tt0099496\n      Elves\n      NaN\n      NaN\n      89.0\n      4.980\n      [Triangle Film Corporation, Fitzgerald Film Co...\n      [elves, cat, experiment, nazi, family secrets,...\n      [Dan Haggerty, Julie Austin, Deanna Lund, Bora...\n      A young woman discovers that she is the focus ...\n    \n    \n      Killer Workout\n      tt0091339\n      Killer Workout\n      NaN\n      NaN\n      84.0\n      5.634\n      [Shapiro Entertainment]\n      [shower, stalker, murder, serial killer, gym, ...\n      [Marcia Karr, David James Campbell, Fritz Matt...\n      Two years ago, a young woman named Valerie was...\n    \n    \n      The Russian Ninja\n      tt0100531\n      The Russian Ninja\n      NaN\n      NaN\n      87.0\n      1.798\n      [Swedish Action Film Force]\n      [kidnapping, ninja]\n      [Frederick Offrein, Helle Michaelsen, Mats Hud...\n      An ex-mercenary is enlisted back into the life...\n    \n    \n      Stone Cold\n      tt0431420\n      Stone Cold\n      NaN\n      NaN\n      87.0\n      13.679\n      [Brandman Productions, Sony Pictures Televisio...\n      [cop, new england]\n      [Tom Selleck, Jane Adams, Viola Davis, Shawn R...\n      Jesse Stone is a former L.A. homicide detectiv...\n    \n  \n\n\n\n\nΚάθε ταινία έχει πολλαπλές τιμές σε κάθε πεδίο. Για την επεξεργασία όλων των πληροφοριών θα πρέπει ή να δημιουργήσουμε μία βάση sql με πολλαπλούς πίνακες ή να δημιουργήσουμε διαφορετικά αρχεία csv που θα μπορούν να συνδυαστούν μεσω του κοινού πεδίου imdb_id. Παρακάτω παρουσιάζεται η δεύτερη μέθοδος. Τα αρχεία θα συνδυαστούν αργότερα στο πρόγραμμα Tableau μέσω των data relationships.\n\n# Αρχείο companies.csv που θα περιέχει τις πληροφορίες των εταιριών παραγωγής\nif not path.exists(path.join('datasets','companies.csv')):\n    companies = (movies_tmdb[['imdb_id', 'production_companies']]\n                 .reset_index()\n                 .rename(columns={'index':'title'})\n                 .explode(column='production_companies')\n                 .dropna().reset_index(drop=True)).copy()\n    companies.to_csv(path.join('datasets','companies.csv'), index=False)\n\n\n# Αρχείο cast.csv που θα περιέχει τους ηθοποιούς ανά ταινία\nif not path.exists(path.join('datasets','cast.csv')):\n    cast = (movies_tmdb[['imdb_id', 'cast']]\n             .reset_index()\n             .rename(columns={'index':'title'})\n             .explode(column='cast')\n             .dropna().reset_index(drop=True)).copy()\n    cast.to_csv(path.join('datasets','cast.csv'), index=False)\n\n\n# Αρχείο keywords.csv που θα περιέχει τις λέξεις κλειδιά για την περιγραφή κάθε ταινίας\nif not path.exists(path.join('datasets','keywords.csv')):\n    keywords = (movies_tmdb[['imdb_id', 'keywords']]\n                .reset_index().rename(columns={'index':'title'})\n                .explode(column='keywords')\n                .dropna().reset_index(drop=True).copy())\n    keywords.to_csv(path.join('datasets', 'keywords.csv'), index=False)\n\n\n# Οι υπόλοιπες στήλες θα αποθηκευτούν σε ένα αρχείο με όνομα movies_tmdb.csv\nprint(\"Στήλες: \", movies_tmdb.columns.values)\n\nΣτήλες:  ['imdb_id' 'original_title' 'budget' 'revenue' 'runtime' 'popularity'\n 'production_companies' 'keywords' 'cast' 'overview']\n\n\n\nif not path.exists(path.join('datasets','movies_tmdb.csv')):\n    movies_tmdb.drop(columns=['production_companies', 'keywords', 'cast'], inplace=True)\n    movies_tmdb.to_csv(path.join('datasets','movies_tmdb.csv'))"
  },
  {
    "objectID": "portfolio/filmpit/helper.html#εξαγωγή-πληροφοριών-από-την-ιστοσελίδα-open-movie-database",
    "href": "portfolio/filmpit/helper.html#εξαγωγή-πληροφοριών-από-την-ιστοσελίδα-open-movie-database",
    "title": "Οι ταινίες του Film Pit σε γραφήματα",
    "section": "Εξαγωγή πληροφοριών από την ιστοσελίδα Open Movie Database",
    "text": "Εξαγωγή πληροφοριών από την ιστοσελίδα Open Movie Database\nΑρχικά προσπάθησα να χρησιμοποιήσω την βιβλιοθήκη omdb αλλά τελικά είχα καλύτερα αποτελέσματα με το api της ιστοσελίδας OMDb. Η OMDb παρέχει τις ίδιες πληροφορίες που έχει το IMDB αλλά με ευκολότερη πρόσβαση. Οι πληροφορίες θα αποθηκευτούν στο λεξικό omdb_info και μετέπειτα σε αρχείο movies_omdb.csv με την βοήθεια του pandas.\n\nif path.exists(path.join('datasets','movies_omdb.csv')):\n    movies_omdb = pd.read_csv(path.join('datasets','movies_omdb.csv'), index_col=0)\nelse:\n    from config import config # αποθήκευση των κλειδιών api στο βοηθητικό αρχείο 'config.py'\n    omdb_info = {}\n    for imdb_id in movies_tmdb['imdb_id'].to_list():\n        try:\n            res = requests.get(f\"http://www.omdbapi.com/?i={imdb_id}&apikey={config['omdb_api_key']}\", timeout=3).json()\n            omdb_info[imdb_id] = [\n                res['Year'],\n                res['Rated'],\n                res['Genre'],\n                res['Director'],\n                res['Writer'],\n                res['Language'],\n                res['Country'],\n                res['Awards'],\n                res['Metascore'],\n                res['imdbRating'],\n                res['imdbVotes'],\n                [rating['Value'] for rating in res['Ratings'] if rating['Source'] == 'Rotten Tomatoes']\n            ]\n        except Exception:\n            print(f\"Request for movie id {imdb_id} did not executed.\")\n\n\nif not path.exists(path.join('datasets','movies_omdb.csv')):\n    columns = ['year',\n               'rated',\n               'genre',\n               'director',\n               'writer',\n               'language',\n               'country',\n               'awards',\n               'metascore',\n               'imdb_rating',\n               'imdb_votes',\n               'rotten_rating']\n    movies_omdb = pd.DataFrame.from_dict(omdb_info, orient='index', columns=columns)\n    movies_omdb = movies_omdb.reset_index().rename(columns={'index': 'imdb_id'})\n    movies_omdb.replace({'N/A': np.nan}, inplace=True)\n    \n    # Βελτίωση των αποτελεσμάτων των ratings\n    movies_omdb['imdb_votes'] = movies_omdb['imdb_votes'].str.replace(',', '').astype(int)\n    movies_omdb['rotten_rating'] = movies_omdb.rotten_rating.apply(lambda x: x[0] if x else np.nan).str.replace('%', '').astype(float)\n    movies_omdb['imdb_rating'] = movies_omdb.imdb_rating.astype(float)\n    movies_omdb['metascore'] = movies_omdb.metascore.astype(float)\n\n\nmovies_omdb.sample(5)\n\n\n\n\n\n  \n    \n      \n      imdb_id\n      year\n      rated\n      genre\n      director\n      writer\n      language\n      country\n      awards\n      metascore\n      imdb_rating\n      imdb_votes\n      rotten_rating\n    \n  \n  \n    \n      31\n      tt0072666\n      1971\n      PG\n      Horror, Sci-Fi\n      Don Barton, Arnold Stevens\n      Ron Kivett, Lee O. Larew, Don Barton\n      English\n      United States\n      NaN\n      NaN\n      2.1\n      5150\n      NaN\n    \n    \n      104\n      tt0086434\n      1983\n      R\n      Action, Crime, Drama\n      Fabrizio De Angelis\n      Fabrizio De Angelis, Dardano Sacchetti\n      Italian, English\n      Italy\n      NaN\n      NaN\n      5.2\n      532\n      NaN\n    \n    \n      103\n      tt0089695\n      1985\n      PG\n      Action, Comedy, Crime\n      Corey Yuen\n      See-Yuen Ng, Corey Yuen, Keith W. Strandberg\n      English\n      United States, Hong Kong\n      NaN\n      30.0\n      5.6\n      17361\n      80.0\n    \n    \n      33\n      tt0085811\n      1983\n      PG\n      Action, Adventure, Fantasy\n      Peter Yates\n      Stanford Sherman\n      English\n      United Kingdom, United States\n      1 win & 5 nominations\n      45.0\n      6.1\n      33773\n      NaN\n    \n    \n      81\n      tt0117433\n      1997\n      R\n      Action, Thriller\n      Sidney J. Furie\n      Sidney J. Furie, Greg Mellott\n      English\n      United States, Canada\n      NaN\n      NaN\n      4.5\n      579\n      NaN\n    \n  \n\n\n\n\n\nmovies_omdb.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 114 entries, 0 to 113\nData columns (total 13 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   imdb_id        114 non-null    object \n 1   year           114 non-null    object \n 2   rated          104 non-null    object \n 3   genre          114 non-null    object \n 4   director       114 non-null    object \n 5   writer         113 non-null    object \n 6   language       114 non-null    object \n 7   country        114 non-null    object \n 8   awards         25 non-null     object \n 9   metascore      27 non-null     float64\n 10  imdb_rating    114 non-null    float64\n 11  imdb_votes     114 non-null    int64  \n 12  rotten_rating  46 non-null     float64\ndtypes: float64(3), int64(1), object(9)\nmemory usage: 11.7+ KB\n\n\nΌπως και στο TMDB υπάρχουν πολλαπλές τιμές για κάθε πεδίο της εκάστοτε ταινίας οπότε δημιούργησα μία εφαρμογή που θα αποθηκεύει αυτές τις τιμές σε ξεχωριστό αρχείο csv μαζί με ένα αναγνωριστικό για την κάθε ταινία.\n\ndef create_sup_tables(column, f_name):\n    \"\"\"\n    The fuction picks a dataframe column, splits values by coma,\n    creates different columns and writes the new dataframe\n    to a csv file based on f_name\n    \"\"\"\n    df = movies_omdb[['imdb_id', column]].copy()\n    df[column] = df[column].str.split(',')\n    df = df.explode(column=column)\n    df[column] = df[column].str.strip()\n    df.dropna(inplace=True)\n    df.reset_index(drop=True, inplace=True)\n    df.to_csv(path.join('datasets', f_name), index=False)\n\n\n# Χρήση της εφαρμογής σε 5 στήλες\n\nif not path.exists(path.join('datasets','genres.csv')):\n    create_sup_tables('genre', 'genres.csv')\n    \nif not path.exists(path.join('datasets','writers.csv')):\n    create_sup_tables('writer', 'writers.csv')\n    \nif not path.exists(path.join('datasets','countries.csv')):\n    create_sup_tables('country', 'countries.csv')\n    \nif not path.exists(path.join('datasets','languages.csv')):\n    create_sup_tables('language', 'languages.csv')\n    \nif not path.exists(path.join('datasets','directors.csv')):\n    create_sup_tables('director', 'directors.csv')\n\nΕξάγουμε τις υπόλοιπες στήλες για τις οποίες δεν δημιουργήσαμε ξεχωριστά αρχεία στο αρχείο movies_omdb.csv.\n\nif not path.exists(path.join('datasets','movies_omdb.csv')):\n    movies_omdb.drop(columns=['genre', 'writer', 'country', 'director', 'language'], inplace=True)\n    movies_omdb.to_csv(path.join('datasets','movies_omdb.csv'))\n\nΤέλος ενώνουμε τις πληροφορίες απο τις δύο ιστοσελίδες σε ένα αρχείο που ονομάζεται movies.csv.\n\nmovies = movies_tmdb.reset_index().rename(columns={'index':'title'}).merge(movies_omdb, how='inner', on='imdb_id')\n\nmovies.info()\n\nif not path.exists(path.join('datasets','movies.csv')):\n    movies.to_csv(path.join('datasets','movies.csv'), index=False)\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 114 entries, 0 to 113\nData columns (total 15 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   title           114 non-null    object \n 1   imdb_id         114 non-null    object \n 2   original_title  114 non-null    object \n 3   budget          35 non-null     float64\n 4   revenue         25 non-null     float64\n 5   runtime         113 non-null    float64\n 6   popularity      114 non-null    float64\n 7   overview        114 non-null    object \n 8   year            114 non-null    object \n 9   rated           104 non-null    object \n 10  awards          25 non-null     object \n 11  metascore       27 non-null     float64\n 12  imdb_rating     114 non-null    float64\n 13  imdb_votes      114 non-null    int64  \n 14  rotten_rating   46 non-null     float64\ndtypes: float64(7), int64(1), object(7)\nmemory usage: 14.2+ KB\n\n\nΣτο πρόγραμμα Tableau χρησιμοποίησα το αρχείο movies.csv ως το κύριο αρχείο και το συνδύασα με τα αρχεία:\n\ncast.csv\ncompanies.csv\ncountries.csv\ndirectors.csv\ngenres.csv\nkeywords.csv\nlanguages.csv\nwriters.csv\n\nγια επιπλεον πληροφορίες."
  }
]